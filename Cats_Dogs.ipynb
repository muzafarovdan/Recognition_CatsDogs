{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нейросеть различающая котов и собак\n",
    "https://www.youtube.com/watch?v=WmNmILXPj2Y\n",
    "1:34:34 минута"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\muzaf\\anaconda3\\envs\\dog\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\muzaf\\anaconda3\\envs\\dog\\lib\\site-packages (0.14.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\muzaf\\anaconda3\\envs\\dog\\lib\\site-packages (1.24.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\muzaf\\anaconda3\\envs\\dog\\lib\\site-packages (3.7.1)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\muzaf\\anaconda3\\envs\\dog\\lib\\site-packages (4.7.0.72)\n",
      "Requirement already satisfied: tqdm in c:\\users\\muzaf\\anaconda3\\envs\\dog\\lib\\site-packages (4.65.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\muzaf\\anaconda3\\envs\\dog\\lib\\site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\muzaf\\anaconda3\\envs\\dog\\lib\\site-packages (from torchvision) (2.28.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\muzaf\\anaconda3\\envs\\dog\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\muzaf\\anaconda3\\envs\\dog\\lib\\site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\muzaf\\anaconda3\\envs\\dog\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\muzaf\\anaconda3\\envs\\dog\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\muzaf\\anaconda3\\envs\\dog\\lib\\site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\muzaf\\anaconda3\\envs\\dog\\lib\\site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\muzaf\\anaconda3\\envs\\dog\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\muzaf\\anaconda3\\envs\\dog\\lib\\site-packages (from matplotlib) (5.12.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\muzaf\\anaconda3\\envs\\dog\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\muzaf\\anaconda3\\envs\\dog\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\muzaf\\anaconda3\\envs\\dog\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\muzaf\\anaconda3\\envs\\dog\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\muzaf\\anaconda3\\envs\\dog\\lib\\site-packages (from requests->torchvision) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\muzaf\\anaconda3\\envs\\dog\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\muzaf\\anaconda3\\envs\\dog\\lib\\site-packages (from requests->torchvision) (3.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\muzaf\\anaconda3\\envs\\dog\\lib\\site-packages (from requests->torchvision) (2022.12.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision numpy matplotlib opencv-python tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "import os\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset2class(torch.utils.data.Dataset):\n",
    "    def __init__(self, path_dir1:str, path_dir2:str):\n",
    "        super().__init__()\n",
    "\n",
    "        self.path_dir1 = path_dir1\n",
    "        self.path_dir2 = path_dir2\n",
    "\n",
    "        self.dir1_list = sorted(os.listdir(path_dir1))\n",
    "        self.dir2_list = sorted(os.listdir(path_dir2))\n",
    "    def __len__(self):\n",
    "        return  len(self.dir1_list) + len(self.dir2_list)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        if idx < len(self.dir1_list):\n",
    "            class_id = 0\n",
    "            img_path = os.path.join(self.path_dir1, self.dir1_list[idx])\n",
    "        else:\n",
    "            class_id = 1\n",
    "            idx -= len(self.dir_list)\n",
    "            img_path = os.path.join(self.path_dir2, self.dir2_list[idx])\n",
    "\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        img = img[:, :, ::-1]\n",
    "\n",
    "\n",
    "        img = img.astype(np.float32)\n",
    "        img = img/255\n",
    "\n",
    "        img = cv2.resize(img, (64,64), interpolation = cv2.INTER_AREA)\n",
    "        img.transpose((2, 0, 1))\n",
    "\n",
    "        t_img = torch.from_numpy(img)\n",
    "        t_class_id = torch.tensor(class_id)\n",
    "    \n",
    "        return {'img': t_img, 'label': t_class_id}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dogs_path = 'C:/Users/muzaf/Desktop/SUAI_AI/Cats and Dogs/training_set/dogs/'\n",
    "train_cats_path = 'C:/Users/muzaf/Desktop/SUAI_AI/Cats and Dogs/training_set/cats/'\n",
    "\n",
    "test_dogs_path = 'C:/Users/muzaf/Desktop/SUAI_AI/Cats and Dogs/test_set/dogs/'\n",
    "test_cats_path = 'C:/Users/muzaf/Desktop/SUAI_AI/Cats and Dogs/test_set/cats/'\n",
    "\n",
    "train_ds_catsdogs = Dataset2class(train_dogs_path, train_cats_path)\n",
    "test_ds_catsdogs = Dataset2class(test_dogs_path, test_cats_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_ds_catsdogs, shuffle=True, batch_size = batch_size, num_workers=1, drop_last=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_ds_catsdogs, shuffle=True, batch_size = batch_size, num_workers=1, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet (nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.act = nn.LeakyReLU(0.2)\n",
    "        self.maxpool = nn.MaxPool2d(2,2)\n",
    "        self.conv0 = nn.Conv2d(3,32,3, stride=1, padding=0)\n",
    "        self.conv1 = nn.Conv2d(32,32,3, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32,64,3, stride=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(64,64,3, stride=1, padding=0)\n",
    "        self.conv4 = nn.Conv2d(64,64,3, stride=1, padding=0)\n",
    "\n",
    "        self.adaptivepool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(64,10)\n",
    "        self.linear2 = nn.Linear(10,2)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        out = self.conv0(x)\n",
    "        out = self.act(out)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.act(out)\n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        out = self.conv2(x)\n",
    "        out = self.act(out)\n",
    "        out = self.maxpool(out)\n",
    "       \n",
    "        out = self.conv3(x)\n",
    "        out = self.act(out)\n",
    "\n",
    "        out = self.adaptivepool(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.linear1(out)\n",
    "        out = self.act(out)\n",
    "        out = self.linear2(out)\n",
    "      \n",
    "        print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (act): LeakyReLU(negative_slope=0.2)\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (adaptivepool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear1): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (linear2): Linear(in_features=10, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103168"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in train_loader:\n",
    "    img = sample['img']\n",
    "    label = sample['label']\n",
    "    model(img)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9,0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, label):\n",
    "    answer = F.softmax(pred.detach()).numpy().argmax(1) == label.numpy().argmax(1)\n",
    "    return answer.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_val = 0\n",
    "    acc_val = 0\n",
    "    for sample in (pbar := tqdm(train_loader)):\n",
    "        img, label = sample['img'], sample['label']\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        label = F.one_hot(label, 2).float()\n",
    "        pred = model(img)\n",
    "\n",
    "        loss = loss_fn(pred, label)\n",
    "\n",
    "        loss.backward()\n",
    "        loss_item = loss.item()\n",
    "        loss_val += loss_item\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        acc_current = accuracy(pred, label)\n",
    "        acc_val += acc_current\n",
    "\n",
    "pbar.set_description(f'loss: {loss_item: .5f}\\taccuracy: {acc_current: .3f}')\n",
    "print(loss_val/len(train_loader))\n",
    "print(acc_val/len(train_loader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
